{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89007f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Due to warning when initializing the \"spacy\" tokenizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow logging\n",
    "logging.getLogger('tensorflow').disabled = True  # disable tensorflow warning messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f59bdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d44f096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Now now, don't mistake my niceness for an open...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1 Now now, don't mistake all my niceness somet...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Now now, don't mistake my reply for being open...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>When the good girl meets that good black dick!...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>When the good girl meets this good black dick....</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet sentiment  label\n",
       "0     im getting on borderlands and i will murder yo...  Positive      3\n",
       "1     I am coming to the borders and I will kill you...  Positive      3\n",
       "2     im getting on borderlands and i will kill you ...  Positive      3\n",
       "3     im coming on borderlands and i will murder you...  Positive      3\n",
       "4     im getting on borderlands 2 and i will murder ...  Positive      3\n",
       "...                                                 ...       ...    ...\n",
       "4995  Now now, don't mistake my niceness for an open...  Negative      1\n",
       "4996  1 Now now, don't mistake all my niceness somet...  Negative      1\n",
       "4997  Now now, don't mistake my reply for being open...  Negative      1\n",
       "4998  When the good girl meets that good black dick!...   Neutral      2\n",
       "4999  When the good girl meets this good black dick....   Neutral      2\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('artifacts/train_cleaned.csv')\n",
    "\n",
    "# limit df to 5000\n",
    "df = df[:5000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380244aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accae582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cd46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>special shoutouts to microsoft excel 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Dang there goes my birthday present but maybe ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>It was ab fab seeing the 6 bungalows built in ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.7 million viewers? wtf? and cs:go has more t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet   sentiment  label\n",
       "0    I mentioned on Facebook that I was struggling ...  Irrelevant      0\n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...     Neutral      2\n",
       "2    @Microsoft Why do I pay for WORD when it funct...    Negative      1\n",
       "3    CSGO matchmaking is so full of closet hacking,...    Negative      1\n",
       "4    Now the President is slapping Americans in the...     Neutral      2\n",
       "..                                                 ...         ...    ...\n",
       "495          special shoutouts to microsoft excel 2013    Positive      3\n",
       "496  Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...  Irrelevant      0\n",
       "497  Dang there goes my birthday present but maybe ...    Positive      3\n",
       "498  It was ab fab seeing the 6 bungalows built in ...  Irrelevant      0\n",
       "499  1.7 million viewers? wtf? and cs:go has more t...     Neutral      2\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.read_csv('artifacts/valid_cleaned.csv')\n",
    "\n",
    "df_valid = df_valid[:500]\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150373c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab86e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf9296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "def token_gen(text):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence in a given text and yields the resulting tokens.\n",
    "\n",
    "    Args:\n",
    "        text (list[str]): A list of sentences to tokenize.\n",
    "\n",
    "    Yields:\n",
    "        list[str]: The resulting tokens from each sentence.\n",
    "    \"\"\"\n",
    "    for sent in text:\n",
    "        tokens = tokenizer(sent)\n",
    "        yield tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8f7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(token_gen(df['tweet']),specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])  ## to handel OOV problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89492619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9d07bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type is: <class 'generator'>\n",
      "[48, 147, 211, 16, 47, 7, 48, 98, 1139, 23, 37, 5]\n",
      "[4, 105, 265, 6, 2, 6086, 7, 4, 98, 651, 23, 37, 5]\n",
      "[48, 147, 211, 16, 47, 7, 48, 98, 651, 23, 37, 5]\n",
      "[48, 147, 265, 16, 47, 7, 48, 98, 1139, 23, 37, 5]\n",
      "[48, 147, 211, 16, 47, 41, 7, 48, 98, 1139, 23, 50, 37, 5]\n",
      "[48, 147, 211, 181, 47, 7, 48, 88, 1139, 23, 37, 5]\n",
      "[97, 4, 576, 8, 479, 235, 425, 231, 12, 82, 1, 1, 1, 153, 23, 49, 36, 160, 4, 105, 8, 1421, 91, 498, 7, 639, 10, 79, 11, 20, 163, 370, 1, 97, 4, 693, 6, 178, 320, 8, 3770, 12, 20, 230, 1, 1, 755, 10, 2, 959, 1032, 3763, 2, 3474, 4, 201, 561, 3898, 3, 4186]\n",
      "[97, 4, 576, 8, 900, 11, 235, 335, 231, 12, 82, 18, 153, 23, 49, 36, 160, 24, 4, 54, 8, 434, 26, 13, 498, 7, 639, 10, 79, 11, 20, 163, 370, 5, 4, 693, 6, 178, 8, 3770, 12, 20, 230, 21, 755, 33, 2, 959, 3656, 1452, 6, 2, 3474, 4, 201, 561, 408, 82, 3, 386, 17, 6670]\n",
      "[97, 4, 576, 8, 479, 235, 335, 231, 12, 82, 18, 153, 23, 49, 36, 160, 4, 54, 8, 1421, 26, 13, 498, 7, 639, 10, 79, 11, 20, 163, 370, 1]\n",
      "[97, 4, 576, 8, 479, 235, 425, 231, 12, 82, 1, 1, 1, 153, 23, 49, 36, 160, 4, 105, 8, 1421, 154, 498, 7, 639, 10, 79, 11, 20, 163, 370, 1, 97, 4, 693, 6, 178, 320, 8, 3770, 12, 20, 230, 1, 1, 755, 10, 2, 959, 1032, 3763, 2, 3474, 4, 201, 561, 3898, 3, 4186]\n"
     ]
    }
   ],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "print('data type is:',type(sequence))\n",
    "\n",
    "count=0\n",
    "for ids in sequence:\n",
    "    print([num for num in ids])\n",
    "    count+=1\n",
    "    if count==10:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4f33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f685b996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4737, 48, 38, 4737, 6749, 1393, 38, 8, 966, 646, 38, 489, 6749, 301]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check how \"numericalize_tokens_from_iterator\" works\n",
    "\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator\n",
    "\n",
    "sequence = numericalize_tokens_from_iterator(vocab,[\"hi how are you\", \"what is your name?\"])\n",
    "list(next(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bd5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4d9155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n",
      "tensor([[ 48, 147, 211,  ...,   0,   0,   0],\n",
      "        [  4, 105, 265,  ...,   0,   0,   0],\n",
      "        [ 48, 147, 211,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [393,  75,   5,  ...,   0,   0,   0],\n",
      "        [595,   2,  60,  ...,   0,   0,   0],\n",
      "        [595,   2,  60,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# numericalize tokens from iterator using vocab\n",
    "sequence = numericalize_tokens_from_iterator(vocab=vocab,iterator=token_gen(df['tweet']))\n",
    "\n",
    "# create a list to store tokenized sequences\n",
    "text = []\n",
    "for i in range(len(df)):\n",
    "    x = list(next(sequence))\n",
    "    text.append(x)\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text = pad_sequence([torch.tensor(x) for x in text], batch_first=True, padding_value=0)\n",
    "\n",
    "# restrict the length of every sequence in the padded_text\n",
    "MAX_LENGTH = 100\n",
    "padded_text = padded_text[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text.shape)\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2a132-5b3b-42bf-a67a-e4e684a19895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97111859-673e-4ff5-b3fb-31e368bb7760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b2d687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  48,  147,  211,   16,   47,    7,   48,   98, 1139,   23,   37,    5,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b971650-7761-4472-b165-9bbc50efd39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'will', 'see', 'you', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('I will see you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec5c506c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 98, 74, 23, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab([tokens])\n",
    "\n",
    "vocab(tokenizer('I will see you!')) ## similar to the 'fit_on_texts()' in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4fa961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 98, 74, 23])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(vocab(tokenizer('I will see you')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d703b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf35b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7350"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(vocab.get_stoi())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff74e03-dfec-4371-92cc-961b71ffff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e397feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = torch.nn.Embedding(num_embeddings=len(vocab),embedding_dim=5,padding_idx=0) \n",
    "\n",
    "## if we want to add embedding for a text, we should assign the value to \"num_embeddings\" according to the \n",
    "## max 'integer_id' that is present in the tokenized text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058b61c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6123e-01, -1.3822e+00,  6.2183e-01, -1.7639e+00,  9.9752e-01],\n",
       "        [ 1.5047e+00,  1.4646e+00, -2.0749e+00, -8.0589e-01, -5.6431e-01],\n",
       "        [-5.3838e-04,  4.6979e-02, -1.8258e+00,  1.9002e+00, -3.6944e-01],\n",
       "        [ 1.6585e+00,  1.7957e+00, -1.2168e+00, -5.4170e-01, -3.9997e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.2981e+00, -1.0127e+00, -1.0710e+00, -2.8276e-03,  1.1104e+00]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = embedd(torch.tensor(vocab(tokenizer('I will see you nonsense!'))))\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b2f76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd36066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n",
      "tensor([[[ 1.1493, -1.1865, -0.0466,  0.3617, -0.7821],\n",
      "         [ 0.7771, -0.9686, -1.6340,  0.2386, -0.9245],\n",
      "         [ 2.3880, -0.5202,  0.9542,  1.8797, -0.6081],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.8303,  1.3537, -1.0909,  1.3556, -0.5178],\n",
      "         [ 0.8772,  0.9927, -0.6076, -1.6259, -0.4072],\n",
      "         [-0.4508, -0.7191, -0.6209, -1.0954,  0.1068],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 1.1493, -1.1865, -0.0466,  0.3617, -0.7821],\n",
      "         [ 0.7771, -0.9686, -1.6340,  0.2386, -0.9245],\n",
      "         [ 2.3880, -0.5202,  0.9542,  1.8797, -0.6081],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1642,  0.5439, -0.3517, -0.7291, -1.1401],\n",
      "         [-0.1576,  0.8794,  1.1673,  2.2562,  1.7107],\n",
      "         [ 0.0290,  1.1129, -0.6704,  0.8457,  1.0802],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9870,  0.6900,  0.4340, -1.3178, -1.1269],\n",
      "         [ 1.0924,  1.6878, -2.3816, -0.1601, -1.2557],\n",
      "         [ 0.8391,  1.6704,  0.5251, -1.6782, -0.3833],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9870,  0.6900,  0.4340, -1.3178, -1.1269],\n",
      "         [ 1.0924,  1.6878, -2.3816, -0.1601, -1.2557],\n",
      "         [ 0.8391,  1.6704,  0.5251, -1.6782, -0.3833],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the Embedding module with the correct weight matrix size\n",
    "embedd = torch.nn.Embedding(len(vocab), 5, padding_idx=0)\n",
    "\n",
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text.shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "input_text = embedd(padded_text)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45e68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c24b5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the padded_text and compare it to the expected input shape of the Embedding module\n",
    "print(padded_text[0].shape)\n",
    "# should be: torch.Size([batch_size, sequence_length])\n",
    "\n",
    "# Use the Embedding module with the padded_text\n",
    "embedd(padded_text[0]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52b2d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Now now, don't mistake my niceness for an open...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1 Now now, don't mistake all my niceness somet...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Now now, don't mistake my reply for being open...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>When the good girl meets that good black dick!...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>When the good girl meets this good black dick....</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet sentiment  label\n",
       "0     im getting on borderlands and i will murder yo...  Positive      3\n",
       "1     I am coming to the borders and I will kill you...  Positive      3\n",
       "2     im getting on borderlands and i will kill you ...  Positive      3\n",
       "3     im coming on borderlands and i will murder you...  Positive      3\n",
       "4     im getting on borderlands 2 and i will murder ...  Positive      3\n",
       "...                                                 ...       ...    ...\n",
       "4995  Now now, don't mistake my niceness for an open...  Negative      1\n",
       "4996  1 Now now, don't mistake all my niceness somet...  Negative      1\n",
       "4997  Now now, don't mistake my reply for being open...  Negative      1\n",
       "4998  When the good girl meets that good black dick!...   Neutral      2\n",
       "4999  When the good girl meets this good black dick....   Neutral      2\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "094939e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 48, 147, 211,  ...,   0,   0,   0],\n",
       "        [  4, 105, 265,  ...,   0,   0,   0],\n",
       "        [ 48, 147, 211,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [393,  75,   5,  ...,   0,   0,   0],\n",
       "        [595,   2,  60,  ...,   0,   0,   0],\n",
       "        [595,   2,  60,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padded_text.shape)\n",
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f14ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label =df['label'].to_list()\n",
    "\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bdefcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3,  ..., 1, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label= torch.tensor(label)\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abadbecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12e2c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(label.unique())\n",
    "\n",
    "# Define the RNNClassify module\n",
    "class RNNClassify(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size,batch_first=True)\n",
    "        \n",
    "        # Define the linear layer\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize the weights of the module\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.rnn.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Embed the input\n",
    "        embedded = self.embed(input)\n",
    "        #print('embedded shape:',embedded.shape)\n",
    "        \n",
    "        # Pass the embedded input through the RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #print('rnn output shape:',output.shape)\n",
    "        #print('rnn hidden shape:',hidden.shape)\n",
    "        \n",
    "        output = output[:, -1, :]  # taking last output of RNN\n",
    "        #print('rnn last output shape:',output.shape)\n",
    "        \n",
    "        # Pass the output through the linear layer\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # Return the output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be011b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7350"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37abeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassify(vocab_size=VOCAB_SIZE,embed_dim=100,hidden_size=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3b0e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  48,  147,  211,   16,   47,    7,   48,   98, 1139,   23,   37,    5,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb3df556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84db2f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f20cdc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0631,  1.1285, -0.5048,  0.2922]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f9326e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b953ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48, 147, 211,  ...,   0,   0,   0],\n",
       "        [  4, 105, 265,  ...,   0,   0,   0],\n",
       "        [ 48, 147, 211,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [393,  75,   5,  ...,   0,   0,   0],\n",
       "        [595,   2,  60,  ...,   0,   0,   0],\n",
       "        [595,   2,  60,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eac4a5f3-0fa9-4a1f-b8c9-b754d4a0e3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d04bc449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd4d2b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0631,  1.1285, -0.5048,  0.2922],\n",
       "        [-0.1203,  0.4269, -0.4568, -1.0651],\n",
       "        [-0.0742,  0.6605,  0.0694, -0.7276],\n",
       "        ...,\n",
       "        [ 1.3229,  1.3524,  0.3528,  1.5246],\n",
       "        [-0.3698,  1.1526, -0.3615,  0.1575],\n",
       "        [ 0.3327,  0.9626,  0.5979,  0.3315]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fdcf82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_text).shape            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b46fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bac64cb",
   "metadata": {},
   "source": [
    "### will try `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445860e4",
   "metadata": {},
   "source": [
    "#### first of all, let me fix `(X_train,y_train)` and `(X_test,y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdef9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train\n",
    "\n",
    "X_train,y_train = padded_text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42811656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>special shoutouts to microsoft excel 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Dang there goes my birthday present but maybe ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>It was ab fab seeing the 6 bungalows built in ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.7 million viewers? wtf? and cs:go has more t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet   sentiment  label\n",
       "0    I mentioned on Facebook that I was struggling ...  Irrelevant      0\n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...     Neutral      2\n",
       "2    @Microsoft Why do I pay for WORD when it funct...    Negative      1\n",
       "3    CSGO matchmaking is so full of closet hacking,...    Negative      1\n",
       "4    Now the President is slapping Americans in the...     Neutral      2\n",
       "..                                                 ...         ...    ...\n",
       "495          special shoutouts to microsoft excel 2013    Positive      3\n",
       "496  Dumb Lucky☘️   (Fortnite Montage) youtu.be/psW...  Irrelevant      0\n",
       "497  Dang there goes my birthday present but maybe ...    Positive      3\n",
       "498  It was ab fab seeing the 6 bungalows built in ...  Irrelevant      0\n",
       "499  1.7 million viewers? wtf? and cs:go has more t...     Neutral      2\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744513ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c7b97fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "543dd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_token_ids = []\n",
    "for i in range(len(df_valid)):\n",
    "    token_id = vocab(tokenizer(df_valid['tweet'][i]))\n",
    "    valid_token_ids.append(token_id)\n",
    "    \n",
    "    \n",
    "#valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a43d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "788fbbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 67])\n",
      "tensor([[   4, 4137,   16,  ...,    0,    0,    0],\n",
      "        [5115,    0,   25,  ...,    0,    0,    0],\n",
      "        [   0,  245,   49,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   0,  122, 2122,  ...,    0,    0,    0],\n",
      "        [  55,   32,    0,  ...,    0,    0,    0],\n",
      "        [   0, 1246,  977,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# valid_token_ids = torch.tensor(valid_token_ids) # this will throw an error, because all sequence are not of same length\n",
    "\n",
    "# Pad the sequences to the same length along dimension 0\n",
    "padded_text_valid = pad_sequence([torch.tensor(x) for x in valid_token_ids], batch_first=True, padding_value=0)\n",
    "# here look, <UNK> will be assign to 0 and padding_idx will be assign also 0\n",
    "\n",
    "padded_text_valid = padded_text_valid[:,:MAX_LENGTH]\n",
    "\n",
    "print(padded_text_valid.shape)\n",
    "print(padded_text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28db56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = df_valid['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e354f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_valid = torch.tensor(label_valid)\n",
    "#label_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02ca2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   4, 4137,   16,  ...,    0,    0,    0],\n",
       "         [5115,    0,   25,  ...,    0,    0,    0],\n",
       "         [   0,  245,   49,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   0,  122, 2122,  ...,    0,    0,    0],\n",
       "         [  55,   32,    0,  ...,    0,    0,    0],\n",
       "         [   0, 1246,  977,  ...,    0,    0,    0]]),\n",
       " tensor([0, 2, 1, 1, 2, 1, 3, 3, 3, 1, 3, 3, 1, 2, 1, 3, 3, 1, 3, 1, 1, 2, 0, 1,\n",
       "         2, 2, 1, 0, 0, 1, 3, 3, 1, 3, 1, 2, 2, 0, 3, 2, 3, 2, 2, 2, 3, 2, 1, 1,\n",
       "         1, 2, 3, 1, 1, 3, 3, 3, 3, 3, 1, 0, 1, 3, 3, 0, 1, 2, 1, 0, 2, 1, 3, 1,\n",
       "         1, 3, 3, 0, 3, 0, 2, 2, 2, 3, 3, 2, 3, 2, 1, 0, 1, 2, 2, 1, 3, 0, 0, 1,\n",
       "         1, 1, 2, 3, 2, 1, 3, 3, 2, 3, 2, 3, 1, 2, 2, 2, 1, 2, 1, 2, 2, 3, 3, 2,\n",
       "         1, 1, 3, 1, 2, 1, 3, 2, 1, 2, 0, 3, 2, 3, 3, 0, 2, 2, 0, 0, 0, 2, 2, 0,\n",
       "         0, 0, 3, 2, 3, 0, 3, 1, 2, 2, 2, 0, 2, 1, 2, 3, 1, 2, 1, 0, 0, 0, 2, 1,\n",
       "         1, 1, 3, 3, 3, 2, 2, 3, 0, 2, 2, 2, 3, 2, 1, 1, 2, 3, 3, 0, 0, 2, 3, 3,\n",
       "         2, 0, 2, 1, 1, 1, 1, 3, 2, 2, 3, 3, 3, 3, 1, 3, 3, 0, 2, 0, 1, 1, 0, 0,\n",
       "         1, 3, 3, 1, 0, 1, 3, 3, 1, 0, 0, 3, 3, 1, 3, 0, 2, 0, 0, 1, 2, 2, 3, 1,\n",
       "         0, 0, 3, 3, 0, 0, 2, 3, 1, 1, 3, 3, 3, 3, 2, 2, 3, 1, 2, 3, 2, 1, 2, 2,\n",
       "         1, 3, 3, 0, 1, 2, 0, 3, 2, 0, 1, 2, 1, 3, 3, 1, 1, 1, 3, 1, 2, 3, 2, 2,\n",
       "         1, 3, 1, 3, 1, 0, 2, 2, 3, 1, 2, 1, 0, 3, 1, 3, 0, 3, 3, 3, 3, 3, 1, 1,\n",
       "         3, 1, 2, 2, 2, 3, 0, 2, 3, 0, 1, 2, 2, 0, 2, 2, 0, 1, 3, 1, 1, 0, 0, 3,\n",
       "         0, 3, 2, 2, 0, 0, 1, 1, 1, 2, 3, 0, 2, 1, 3, 0, 2, 1, 1, 1, 3, 2, 2, 0,\n",
       "         1, 3, 3, 0, 2, 0, 3, 2, 2, 3, 3, 1, 2, 3, 1, 2, 1, 0, 1, 3, 3, 0, 3, 3,\n",
       "         2, 1, 2, 0, 0, 3, 2, 3, 1, 1, 1, 0, 3, 2, 3, 0, 1, 2, 0, 1, 3, 3, 3, 3,\n",
       "         2, 2, 0, 2, 1, 3, 3, 2, 1, 3, 2, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 1, 1, 2,\n",
       "         1, 2, 2, 1, 0, 3, 0, 0, 1, 3, 3, 3, 0, 2, 3, 0, 2, 2, 1, 3, 2, 2, 2, 2,\n",
       "         0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 3, 0, 0, 3, 0, 0, 2, 1, 1, 3, 3, 1,\n",
       "         3, 3, 3, 3, 0, 3, 0, 2, 1, 3, 2, 2, 3, 3, 0, 3, 0, 3, 0, 2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test,y_test\n",
    "\n",
    "X_test, y_test = padded_text_valid, label_valid\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c9b8a",
   "metadata": {},
   "source": [
    "#### Now, write the train and test loop for `Batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3f2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # remember it gives logits (row outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18a91fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.6379, Train Accuracy: 0.2374\n",
      "Epoch 1/50, Test Loss: 1.5334, Test Accuracy: 0.2780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2/50, Train Loss: 1.5692, Train Accuracy: 0.2484\n",
      "Epoch 2/50, Test Loss: 1.5297, Test Accuracy: 0.2840\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3/50, Train Loss: 1.5449, Train Accuracy: 0.2578\n",
      "Epoch 3/50, Test Loss: 1.5032, Test Accuracy: 0.2940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4/50, Train Loss: 1.5004, Train Accuracy: 0.2616\n",
      "Epoch 4/50, Test Loss: 1.4785, Test Accuracy: 0.2840\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5/50, Train Loss: 1.4694, Train Accuracy: 0.2836\n",
      "Epoch 5/50, Test Loss: 1.4399, Test Accuracy: 0.2900\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6/50, Train Loss: 1.4427, Train Accuracy: 0.2922\n",
      "Epoch 6/50, Test Loss: 1.4320, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7/50, Train Loss: 1.4252, Train Accuracy: 0.3034\n",
      "Epoch 7/50, Test Loss: 1.4256, Test Accuracy: 0.3100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8/50, Train Loss: 1.4077, Train Accuracy: 0.3012\n",
      "Epoch 8/50, Test Loss: 1.4206, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9/50, Train Loss: 1.3929, Train Accuracy: 0.3048\n",
      "Epoch 9/50, Test Loss: 1.4201, Test Accuracy: 0.2900\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10/50, Train Loss: 1.3825, Train Accuracy: 0.3216\n",
      "Epoch 10/50, Test Loss: 1.4226, Test Accuracy: 0.2820\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 11/50, Train Loss: 1.3755, Train Accuracy: 0.3370\n",
      "Epoch 11/50, Test Loss: 1.4216, Test Accuracy: 0.2800\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 12/50, Train Loss: 1.3683, Train Accuracy: 0.3462\n",
      "Epoch 12/50, Test Loss: 1.4170, Test Accuracy: 0.2900\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 13/50, Train Loss: 1.3609, Train Accuracy: 0.3584\n",
      "Epoch 13/50, Test Loss: 1.4151, Test Accuracy: 0.2920\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 14/50, Train Loss: 1.3568, Train Accuracy: 0.3618\n",
      "Epoch 14/50, Test Loss: 1.4157, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 15/50, Train Loss: 1.3507, Train Accuracy: 0.3666\n",
      "Epoch 15/50, Test Loss: 1.4178, Test Accuracy: 0.3000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 16/50, Train Loss: 1.3456, Train Accuracy: 0.3704\n",
      "Epoch 16/50, Test Loss: 1.4187, Test Accuracy: 0.3020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 17/50, Train Loss: 1.3420, Train Accuracy: 0.3750\n",
      "Epoch 17/50, Test Loss: 1.4165, Test Accuracy: 0.2940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 18/50, Train Loss: 1.3376, Train Accuracy: 0.3776\n",
      "Epoch 18/50, Test Loss: 1.4157, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 19/50, Train Loss: 1.3350, Train Accuracy: 0.3802\n",
      "Epoch 19/50, Test Loss: 1.4176, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 20/50, Train Loss: 1.3311, Train Accuracy: 0.3834\n",
      "Epoch 20/50, Test Loss: 1.4179, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 21/50, Train Loss: 1.3283, Train Accuracy: 0.3876\n",
      "Epoch 21/50, Test Loss: 1.4154, Test Accuracy: 0.2920\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 22/50, Train Loss: 1.3253, Train Accuracy: 0.3824\n",
      "Epoch 22/50, Test Loss: 1.4157, Test Accuracy: 0.2900\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 23/50, Train Loss: 1.3217, Train Accuracy: 0.3854\n",
      "Epoch 23/50, Test Loss: 1.4163, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 24/50, Train Loss: 1.3197, Train Accuracy: 0.3910\n",
      "Epoch 24/50, Test Loss: 1.4139, Test Accuracy: 0.2900\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 25/50, Train Loss: 1.3167, Train Accuracy: 0.3876\n",
      "Epoch 25/50, Test Loss: 1.4138, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 26/50, Train Loss: 1.3133, Train Accuracy: 0.3930\n",
      "Epoch 26/50, Test Loss: 1.4126, Test Accuracy: 0.3000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 27/50, Train Loss: 1.3105, Train Accuracy: 0.3924\n",
      "Epoch 27/50, Test Loss: 1.4099, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 28/50, Train Loss: 1.3083, Train Accuracy: 0.3934\n",
      "Epoch 28/50, Test Loss: 1.4110, Test Accuracy: 0.3000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 29/50, Train Loss: 1.3076, Train Accuracy: 0.3944\n",
      "Epoch 29/50, Test Loss: 1.4050, Test Accuracy: 0.3020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 30/50, Train Loss: 1.3106, Train Accuracy: 0.3888\n",
      "Epoch 30/50, Test Loss: 1.4113, Test Accuracy: 0.3020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 31/50, Train Loss: 1.3090, Train Accuracy: 0.3938\n",
      "Epoch 31/50, Test Loss: 1.4054, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 32/50, Train Loss: 1.2973, Train Accuracy: 0.4050\n",
      "Epoch 32/50, Test Loss: 1.4066, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 33/50, Train Loss: 1.2923, Train Accuracy: 0.4080\n",
      "Epoch 33/50, Test Loss: 1.4093, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 34/50, Train Loss: 1.2931, Train Accuracy: 0.4074\n",
      "Epoch 34/50, Test Loss: 1.4030, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 35/50, Train Loss: 1.2966, Train Accuracy: 0.4006\n",
      "Epoch 35/50, Test Loss: 1.4102, Test Accuracy: 0.2940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 36/50, Train Loss: 1.2911, Train Accuracy: 0.4124\n",
      "Epoch 36/50, Test Loss: 1.4052, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 37/50, Train Loss: 1.2776, Train Accuracy: 0.4192\n",
      "Epoch 37/50, Test Loss: 1.4061, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 38/50, Train Loss: 1.2708, Train Accuracy: 0.4264\n",
      "Epoch 38/50, Test Loss: 1.4101, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 39/50, Train Loss: 1.2735, Train Accuracy: 0.4302\n",
      "Epoch 39/50, Test Loss: 1.4020, Test Accuracy: 0.3020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 40/50, Train Loss: 1.2885, Train Accuracy: 0.4074\n",
      "Epoch 40/50, Test Loss: 1.4135, Test Accuracy: 0.2960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 41/50, Train Loss: 1.2763, Train Accuracy: 0.4174\n",
      "Epoch 41/50, Test Loss: 1.4083, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 42/50, Train Loss: 1.2537, Train Accuracy: 0.4376\n",
      "Epoch 42/50, Test Loss: 1.4070, Test Accuracy: 0.3120\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 43/50, Train Loss: 1.2530, Train Accuracy: 0.4346\n",
      "Epoch 43/50, Test Loss: 1.4157, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 44/50, Train Loss: 1.2662, Train Accuracy: 0.4298\n",
      "Epoch 44/50, Test Loss: 1.4073, Test Accuracy: 0.2980\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 45/50, Train Loss: 1.2506, Train Accuracy: 0.4326\n",
      "Epoch 45/50, Test Loss: 1.4084, Test Accuracy: 0.3120\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 46/50, Train Loss: 1.2401, Train Accuracy: 0.4510\n",
      "Epoch 46/50, Test Loss: 1.4153, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 47/50, Train Loss: 1.2403, Train Accuracy: 0.4488\n",
      "Epoch 47/50, Test Loss: 1.4097, Test Accuracy: 0.3020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 48/50, Train Loss: 1.2575, Train Accuracy: 0.4186\n",
      "Epoch 48/50, Test Loss: 1.4080, Test Accuracy: 0.3060\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 49/50, Train Loss: 1.2390, Train Accuracy: 0.4538\n",
      "Epoch 49/50, Test Loss: 1.4225, Test Accuracy: 0.2940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 50/50, Train Loss: 1.2959, Train Accuracy: 0.3892\n",
      "Epoch 50/50, Test Loss: 1.4238, Test Accuracy: 0.2920\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here, we will use Batch Gradient Descent, BUT, generally we prefer Mini-Batch Gradient Descent\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc = 0,0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    X_train,y_train = X_train.to(device), y_train.to(device)\n",
    "    \n",
    "    y_logits = model(X_train)\n",
    "    #print('shape of y_logits:',y_logits.shape)\n",
    "\n",
    "    \n",
    "    # Compute loss with one-hot encoded targets\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    \n",
    "    train_loss += loss\n",
    "    train_acc += (y_logits.argmax(1) == y_train).sum().item() / len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        test_loss,test_acc = 0,0\n",
    "    \n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        \n",
    "        y_logits = model(X_test)\n",
    "\n",
    "        # Compute loss with one-hot encoded targets\n",
    "        loss = loss_fn(y_logits, y_test)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "            \n",
    "        # Compute accuracy\n",
    "        test_preds = y_logits.argmax(dim=1)\n",
    "        test_acc += (test_preds == y_test).sum().item() / len(y_test)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "        \n",
    "        print('--'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935adac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9f2fd-1ad6-436e-b164-f151b45053f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
